# ğŸ§  RAG Backend (Fully Local)

A scalable, fully local Retrieval-Augmented Generation (RAG) backend system designed to ingest large document collections and answer queries with high factual grounding and minimal hallucination.

> âš ï¸ Note: OpenAI API is intentionally not used because it is paid.  
> This system is fully local and cost-free using open-source models.

---
    
## ğŸš€ Tech Stack

| Layer          | Tool Used |
|---------------|-----------|
| API           | FastAPI |
| LLM           | Llama 3 (via Ollama) |
| Embeddings    | BGE-small / all-MiniLM |
| Vector Search | FAISS (HNSW) |
| Keyword Search| BM25 |
| Metadata      | SQLite / PostgreSQL |
| Container     | Docker |

---

## ğŸ— Architecture

### Ingestion Pipeline
Documents â†’ Chunking â†’ Embeddings â†’ FAISS + BM25 â†’ Metadata Store

### Query Pipeline
User Query â†’ Query Embedding â†’ Hybrid Retrieval â†’ Context Filtering â†’ Llama 3 â†’ Response

---

## ğŸ” Key Engineering Decisions

- Hybrid search (Dense + BM25) to reduce semantic drift
- HNSW index for scalable vector retrieval
- Strict context-grounded prompting to control hallucination
- Modular backend structure for production scaling
- Fully local models to eliminate API cost and rate limits

---

## ğŸ“Š Evaluation

Supports:
- Recall@k
- Precision@k
- Context relevance scoring
- Hallucination detection

---

## ğŸ›¡ Hallucination Control

LLM is forced to:
- Use only retrieved context
- Return â€œInsufficient informationâ€ if unsupported

---

## ğŸ¯ Why No OpenAI API?

OpenAI API is paid and rate-limited.  
This project is intentionally designed to be:

- Fully local
- Cost-free
- Scalable
- Production-ready

---

## ğŸ§  What This Demonstrates

- Retrieval system design
- Vector indexing optimization
- Hybrid ranking strategies
- Backend modular architecture
- Practical RAG production engineering

---

## â–¶ Run Locally

```bash
uvicorn app.main:app --reload